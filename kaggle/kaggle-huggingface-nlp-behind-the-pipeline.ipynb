{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Preprocessing with a tokenizer\nfrom transformers import AutoTokenizer\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-12T13:26:35.554402Z","iopub.execute_input":"2024-07-12T13:26:35.554807Z","iopub.status.idle":"2024-07-12T13:26:43.418966Z","shell.execute_reply.started":"2024-07-12T13:26:35.554750Z","shell.execute_reply":"2024-07-12T13:26:43.417702Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43003b53bfd34d5aa07e4750b01e3d88"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd819a20a76c4e508ba397aef5cee8ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2fc71975a0d42a687d2b3943700225c"}},"metadata":{}}]},{"cell_type":"code","source":"raw_inputs = [\n    \"I fell for the hype. When shopping for a new TV I made the mistake of getting swayed by the price and the salesperson influence. I wound up buying a 65 inches OLED LG TV. And once installed my troubles started. Right at the outset I noticed the slowness of downloading, opening apps and streaming. Of course I contacted support and after hours and days of arguing they agreed to send a repair crew. It was determined that the motherboard was faulty and replaced it.\",\n    \"My 32 GB Picture Keeper suddenly stopped working. Fearing the worst case scenario. I contacted P.K. Tech Team, Aaron P sent a Link to recover my pictures. It worked, over 15,000 images recovered. Many Thanks Aaron P. I thoroughly recommend P.K.\",\n]\n\ninputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\nprint(inputs)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:26:43.421182Z","iopub.execute_input":"2024-07-12T13:26:43.421677Z","iopub.status.idle":"2024-07-12T13:26:43.476211Z","shell.execute_reply.started":"2024-07-12T13:26:43.421642Z","shell.execute_reply":"2024-07-12T13:26:43.475052Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[  101,  1045,  3062,  2005,  1996,  1044, 18863,  1012,  2043,  6023,\n          2005,  1037,  2047,  2694,  1045,  2081,  1996,  6707,  1997,  2893,\n         20122,  2011,  1996,  3976,  1998,  1996,  4341, 27576,  3747,  1012,\n          1045,  6357,  2039,  9343,  1037,  3515,  5282, 15589,  2094,  1048,\n          2290,  2694,  1012,  1998,  2320,  5361,  2026, 13460,  2318,  1012,\n          2157,  2012,  1996, 26674,  1045,  4384,  1996,  4030,  2791,  1997,\n          8816,  2075,  1010,  3098, 18726,  1998, 11058,  1012,  1997,  2607,\n          1045, 11925,  2490,  1998,  2044,  2847,  1998,  2420,  1997,  9177,\n          2027,  3530,  2000,  4604,  1037,  7192,  3626,  1012,  2009,  2001,\n          4340,  2008,  1996,  2388,  6277,  2001, 28927,  1998,  2999,  2009,\n          1012,   102],\n        [  101,  2026,  3590, 16351,  3861, 10684,  3402,  3030,  2551,  1012,\n         14892,  1996,  5409,  2553, 11967,  1012,  1045, 11925,  1052,  1012,\n          1047,  1012,  6627,  2136,  1010,  7158,  1052,  2741,  1037,  4957,\n          2000,  8980,  2026,  4620,  1012,  2009,  2499,  1010,  2058,  2321,\n          1010,  2199,  4871,  6757,  1012,  2116,  4283,  7158,  1052,  1012,\n          1045, 12246, 16755,  1052,  1012,  1047,  1012,   102,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0]])}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Going through the model\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(checkpoint)\noutputs = model(**inputs)\nprint(outputs.last_hidden_state.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:26:43.477561Z","iopub.execute_input":"2024-07-12T13:26:43.477918Z","iopub.status.idle":"2024-07-12T13:26:47.935079Z","shell.execute_reply.started":"2024-07-12T13:26:43.477888Z","shell.execute_reply":"2024-07-12T13:26:47.933859Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26bd5cadac8b4a2f96464270c51ef092"}},"metadata":{}},{"name":"stdout","text":"torch.Size([2, 102, 768])\n","output_type":"stream"}]},{"cell_type":"code","source":"# There are many different architectures available in ðŸ¤— Transformers, with each one designed around tackling a specific task. Here is a non-exhaustive list:\n\n# *Model (retrieve the hidden states)\n# *ForCausalLM\n# *ForMaskedLM\n# *ForMultipleChoice\n# *ForQuestionAnswering\n# *ForSequenceClassification\n# *ForTokenClassification\n# and others ðŸ¤—\n","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:26:47.938277Z","iopub.execute_input":"2024-07-12T13:26:47.939062Z","iopub.status.idle":"2024-07-12T13:26:47.944007Z","shell.execute_reply.started":"2024-07-12T13:26:47.939018Z","shell.execute_reply":"2024-07-12T13:26:47.942870Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). \n# So, we wonâ€™t actually use the AutoModel class, but AutoModelForSequenceClassification:\n\nfrom transformers import AutoModelForSequenceClassification\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\noutputs = model(**inputs)\nprint(outputs.logits.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:26:47.945682Z","iopub.execute_input":"2024-07-12T13:26:47.946393Z","iopub.status.idle":"2024-07-12T13:26:48.622309Z","shell.execute_reply.started":"2024-07-12T13:26:47.946351Z","shell.execute_reply":"2024-07-12T13:26:48.620883Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"torch.Size([2, 2])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Postprocessing the output\n\nprint(outputs.logits)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:26:48.624166Z","iopub.execute_input":"2024-07-12T13:26:48.624521Z","iopub.status.idle":"2024-07-12T13:26:48.648639Z","shell.execute_reply.started":"2024-07-12T13:26:48.624488Z","shell.execute_reply":"2024-07-12T13:26:48.647512Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"tensor([[ 4.3398, -3.5608],\n        [-2.4272,  2.5886]], grad_fn=<AddmmBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"# To be converted to probabilities, they need to go through a SoftMax layer\nimport torch\n\npredictions = torch.nn.functional.softmax(outputs.logits, dim=1)\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:26:48.650108Z","iopub.execute_input":"2024-07-12T13:26:48.650420Z","iopub.status.idle":"2024-07-12T13:26:48.657503Z","shell.execute_reply.started":"2024-07-12T13:26:48.650392Z","shell.execute_reply":"2024-07-12T13:26:48.656289Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"tensor([[9.9963e-01, 3.7040e-04],\n        [6.5886e-03, 9.9341e-01]], grad_fn=<SoftmaxBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"# To get the labels corresponding to each position, we can inspect the id2label attribute of the model config\nmodel.config.id2label","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:26:48.659112Z","iopub.execute_input":"2024-07-12T13:26:48.659817Z","iopub.status.idle":"2024-07-12T13:26:48.669036Z","shell.execute_reply.started":"2024-07-12T13:26:48.659762Z","shell.execute_reply":"2024-07-12T13:26:48.667807Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{0: 'NEGATIVE', 1: 'POSITIVE'}"},"metadata":{}}]},{"cell_type":"code","source":"# Now we can conclude that the model predicted the following:\n# First sentence: NEGATIVE: 0.99963, POSITIVE: 0.00037\n# Second sentence: NEGATIVE: 0.00659, POSITIVE: 0.99341","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:26:48.670378Z","iopub.execute_input":"2024-07-12T13:26:48.670690Z","iopub.status.idle":"2024-07-12T13:26:48.677235Z","shell.execute_reply.started":"2024-07-12T13:26:48.670663Z","shell.execute_reply":"2024-07-12T13:26:48.675898Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Creating a Transformer\n# Creating a model from the default configuration initializes it with random values:\nfrom transformers import BertConfig, BertModel\n# Building the config\nconfig = BertConfig()\n\n# Building the model from the config\n# Model is randomly initialized!\nmodel = BertModel(config)\n\n# The configuration contains many attributes that are used to build the model:\nprint(config)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:26:48.682101Z","iopub.execute_input":"2024-07-12T13:26:48.682532Z","iopub.status.idle":"2024-07-12T13:26:50.683376Z","shell.execute_reply.started":"2024-07-12T13:26:48.682499Z","shell.execute_reply":"2024-07-12T13:26:50.682168Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"BertConfig {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.41.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loading a Transformer model that is already trained is simple â€” we can do this using the from_pretrained() method:\nfrom transformers import BertModel\nmodel = BertModel.from_pretrained(\"bert-base-cased\")\n\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:26:50.685085Z","iopub.execute_input":"2024-07-12T13:26:50.685524Z","iopub.status.idle":"2024-07-12T13:27:00.286617Z","shell.execute_reply.started":"2024-07-12T13:26:50.685483Z","shell.execute_reply":"2024-07-12T13:27:00.285457Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f26d264969c4aa8b47e6a094e848880"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0d09bdf81414ae5a266c540f15594d7"}},"metadata":{}},{"name":"stdout","text":"BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x BertLayer(\n        (attention): BertAttention(\n          (self): BertSdpaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(model.config)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:27:00.288223Z","iopub.execute_input":"2024-07-12T13:27:00.288647Z","iopub.status.idle":"2024-07-12T13:27:00.295881Z","shell.execute_reply.started":"2024-07-12T13:27:00.288613Z","shell.execute_reply":"2024-07-12T13:27:00.294666Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"BertConfig {\n  \"_name_or_path\": \"bert-base-cased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.41.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 28996\n}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Using a Transformer model for inference\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\nsequences = [\"Hello!\", \"Cool.\", \"Nice!\"] # changing word might cause error due to tensor dim.\n\nencoded_sequences = tokenizer(sequences)\nprint(encoded_sequences)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:27:00.297432Z","iopub.execute_input":"2024-07-12T13:27:00.297878Z","iopub.status.idle":"2024-07-12T13:27:03.743342Z","shell.execute_reply.started":"2024-07-12T13:27:00.297829Z","shell.execute_reply":"2024-07-12T13:27:03.742140Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0914be41c01b4fe896844b8561704c75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e297a3e94b4457cac35e5fe28088b7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54bbda2cbc4744948f13ff691ce9c8ad"}},"metadata":{}},{"name":"stdout","text":"{'input_ids': [[101, 8667, 106, 102], [101, 13297, 119, 102], [101, 8835, 106, 102]], 'token_type_ids': [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]}\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nprint(encoded_sequences['input_ids'])\nmodel_inputs = torch.tensor(encoded_sequences['input_ids'])\nprint(\"--------output-------\")\noutput = model(model_inputs)\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:27:03.744558Z","iopub.execute_input":"2024-07-12T13:27:03.744925Z","iopub.status.idle":"2024-07-12T13:27:03.846966Z","shell.execute_reply.started":"2024-07-12T13:27:03.744894Z","shell.execute_reply":"2024-07-12T13:27:03.845778Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"[[101, 8667, 106, 102], [101, 13297, 119, 102], [101, 8835, 106, 102]]\n--------output-------\nBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.6283,  0.2166,  0.5605,  ...,  0.0136,  0.6158, -0.1712],\n         [ 0.6108, -0.2253,  0.9263,  ..., -0.3028,  0.4500, -0.0714],\n         [ 0.8040,  0.1809,  0.7076,  ..., -0.0685,  0.4837, -0.0774],\n         [ 1.3290,  0.2360,  0.4567,  ...,  0.1509,  0.9621, -0.4841]],\n\n        [[ 0.3128,  0.1718,  0.2099,  ..., -0.0721,  0.4919, -0.1383],\n         [ 0.1545, -0.3757,  0.7187,  ..., -0.3130,  0.2822,  0.1883],\n         [ 0.4123,  0.3721,  0.5484,  ...,  0.0788,  0.5681, -0.2757],\n         [ 0.8356,  0.3964, -0.4121,  ...,  0.1838,  1.6365, -0.4806]],\n\n        [[ 0.5399,  0.2564,  0.2511,  ..., -0.1760,  0.6063, -0.1803],\n         [ 0.2609, -0.3164,  0.5548,  ..., -0.3439,  0.3909,  0.0900],\n         [ 0.5161,  0.0721,  0.5606,  ...,  0.0077,  0.3685, -0.2272],\n         [ 0.6560,  0.8475, -0.1606,  ..., -0.0468,  1.6309, -0.5047]]],\n       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.7105,  0.4876,  0.9999,  ...,  1.0000, -0.9179,  0.9894],\n        [-0.7731,  0.5619,  1.0000,  ...,  1.0000, -0.8397,  0.9944],\n        [-0.7594,  0.5645,  1.0000,  ...,  1.0000, -0.9015,  0.9969]],\n       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tokenizers\n\n# Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except we use the BertTokenizer class:\nfrom transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n\n# Similar to AutoModel, the AutoTokenizer class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint:\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\ntokenizer(\"Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:27:03.848433Z","iopub.execute_input":"2024-07-12T13:27:03.848890Z","iopub.status.idle":"2024-07-12T13:27:04.373715Z","shell.execute_reply.started":"2024-07-12T13:27:03.848847Z","shell.execute_reply":"2024-07-12T13:27:04.372550Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [101, 10605, 17465, 1103, 139, 9637, 1942, 22559, 17260, 3972, 1114, 1103, 1269, 4031, 7587, 1112, 139, 9637, 1942, 1110, 1694, 1103, 1269, 1236, 1112, 10745, 1103, 2235, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"},"metadata":{}}]},{"cell_type":"code","source":"# Encoding\n\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\nsequence = [\"Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model.\", \n            \"AutoTokenizer class will grab the proper tokenizer class in the library based on the checkpoint name.\"]\ntokens = tokenizer.tokenize(sequence)\nprint(tokens)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:27:04.375422Z","iopub.execute_input":"2024-07-12T13:27:04.376055Z","iopub.status.idle":"2024-07-12T13:27:04.651971Z","shell.execute_reply.started":"2024-07-12T13:27:04.376010Z","shell.execute_reply":"2024-07-12T13:27:04.650886Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"['Lo', '##ading', 'the', 'B', '##ER', '##T', 'token', '##izer', 'trained', 'with', 'the', 'same', 'check', '##point', 'as', 'B', '##ER', '##T', 'is', 'done', 'the', 'same', 'way', 'as', 'loading', 'the', 'model', '.', 'Auto', '##T', '##oken', '##izer', 'class', 'will', 'grab', 'the', 'proper', 'token', '##izer', 'class', 'in', 'the', 'library', 'based', 'on', 'the', 'check', '##point', 'name', '.']\n","output_type":"stream"}]},{"cell_type":"code","source":"# From tokens to input IDs\nids = tokenizer.convert_tokens_to_ids(tokens)\nprint(ids)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:27:04.653330Z","iopub.execute_input":"2024-07-12T13:27:04.653678Z","iopub.status.idle":"2024-07-12T13:27:04.659585Z","shell.execute_reply.started":"2024-07-12T13:27:04.653648Z","shell.execute_reply":"2024-07-12T13:27:04.658420Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[10605, 17465, 1103, 139, 9637, 1942, 22559, 17260, 3972, 1114, 1103, 1269, 4031, 7587, 1112, 139, 9637, 1942, 1110, 1694, 1103, 1269, 1236, 1112, 10745, 1103, 2235, 119, 12983, 1942, 27443, 17260, 1705, 1209, 6387, 1103, 4778, 22559, 17260, 1705, 1107, 1103, 3340, 1359, 1113, 1103, 4031, 7587, 1271, 119]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Decoding\n\nstring = tokenizer.decode(ids)\nprint(string)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:27:04.660782Z","iopub.execute_input":"2024-07-12T13:27:04.661138Z","iopub.status.idle":"2024-07-12T13:27:19.219604Z","shell.execute_reply.started":"2024-07-12T13:27:04.661108Z","shell.execute_reply":"2024-07-12T13:27:19.218328Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"2024-07-12 13:27:06.846143: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-12 13:27:06.846294: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-12 13:27:07.011875: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model. AutoTokenizer class will grab the proper tokenizer class in the library based on the checkpoint name.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Models expect a batch of inputs\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n\nsequence = \"Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model.\"\n\ntokens = tokenizer.tokenize(sequence)\nids = tokenizer.convert_tokens_to_ids(tokens)\ninput_ids = torch.tensor(ids) # a single sequence.\n# This line will fail because we send a single sequence but the model expect list of sequence. We have to add [] to change ids to the lis of ids.\n# model(input_ids)\ninput_ids = torch.tensor([ids]) # list of ids by adding []\nprint(\"Input IDs = \", input_ids)\n\noutput = model(input_ids)\nprint(\"Logits = \", output.logits)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:44:01.497885Z","iopub.execute_input":"2024-07-12T13:44:01.498579Z","iopub.status.idle":"2024-07-12T13:44:02.153343Z","shell.execute_reply.started":"2024-07-12T13:44:01.498543Z","shell.execute_reply":"2024-07-12T13:44:02.152011Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Input IDs =  tensor([[10578,  1996, 14324, 19204, 17629,  4738,  2007,  1996,  2168, 26520,\n          2004, 14324,  2003,  2589,  1996,  2168,  2126,  2004, 10578,  1996,\n          2944,  1012]])\nLogits =  tensor([[ 3.8296, -3.1792]], grad_fn=<AddmmBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Batching is the act of sending multiple sentences through the model, all at once. If you only have one sentence, you can just build a batch with a single sequence:\nbatched_ids = [ids, ids]\nbatched_input_ids = torch.tensor(batched_ids)\nprint(\"Bached Input IDs = \", batched_input_ids)\n\nbatched_output = model(batched_input_ids)\nprint(\"Bached Logits = \", batched_output.logits)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:55:10.002360Z","iopub.execute_input":"2024-07-12T13:55:10.002753Z","iopub.status.idle":"2024-07-12T13:55:10.071667Z","shell.execute_reply.started":"2024-07-12T13:55:10.002713Z","shell.execute_reply":"2024-07-12T13:55:10.070447Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Bached Input IDs =  tensor([[10578,  1996, 14324, 19204, 17629,  4738,  2007,  1996,  2168, 26520,\n          2004, 14324,  2003,  2589,  1996,  2168,  2126,  2004, 10578,  1996,\n          2944,  1012],\n        [10578,  1996, 14324, 19204, 17629,  4738,  2007,  1996,  2168, 26520,\n          2004, 14324,  2003,  2589,  1996,  2168,  2126,  2004, 10578,  1996,\n          2944,  1012]])\nBached Logits =  tensor([[ 3.8296, -3.1792],\n        [ 3.8296, -3.1792]], grad_fn=<AddmmBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Attention masks\n# Attention masks are tensors with the exact same shape as the input IDs tensor, \n# filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, \n# and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model).\nbatched_ids = [\n    [200, 200, 200],\n    [200, 200, tokenizer.pad_token_id],\n]\n\nattention_mask = [\n    [1, 1, 1],\n    [1, 1, 0],\n]\n\noutputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\nprint(outputs.logits)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T14:02:30.696213Z","iopub.execute_input":"2024-07-12T14:02:30.696660Z","iopub.status.idle":"2024-07-12T14:02:30.749640Z","shell.execute_reply.started":"2024-07-12T14:02:30.696624Z","shell.execute_reply":"2024-07-12T14:02:30.748492Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"tensor([[ 1.5694, -1.3895],\n        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Putting it all together\n# In the last few sections, weâ€™ve been trying our best to do most of the work by hand. \n# Weâ€™ve explored how tokenizers work and looked at tokenization, conversion to input IDs, padding, truncation, and attention masks.\n# However, as we saw in section 2, the ðŸ¤— Transformers API can handle all of this for us with a high-level function that weâ€™ll dive into here. \n# When you call your tokenizer directly on the sentence, you get back inputs that are ready to pass through your model:\n\nfrom transformers import AutoTokenizer\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\nsequences = [\"Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model.\", \n            \"AutoTokenizer class will grab the proper tokenizer class in the library based on the checkpoint name.\"]\nmodel_inputs = tokenizer(sequences)\n\n# Will pad the sequences up to the maximum sequence length\nmodel_inputs = tokenizer(sequences, padding=\"longest\")\nprint(\"Longest = \", model_inputs)\nprint(\"---------------------------------\")\n\n# Will pad the sequences up to the model max length\n# (512 for BERT or DistilBERT)\nmodel_inputs = tokenizer(sequences, padding=\"max_length\")\nprint(\"max_length = \", model_inputs)\nprint(\"---------------------------------\")\n\n# Will pad the sequences up to the specified max length\nmodel_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)\nprint(\"specified max length = \", model_inputs)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T14:12:29.915451Z","iopub.execute_input":"2024-07-12T14:12:29.915893Z","iopub.status.idle":"2024-07-12T14:12:30.208474Z","shell.execute_reply.started":"2024-07-12T14:12:29.915857Z","shell.execute_reply":"2024-07-12T14:12:30.207153Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Longest =  {'input_ids': [[101, 10578, 1996, 14324, 19204, 17629, 4738, 2007, 1996, 2168, 26520, 2004, 14324, 2003, 2589, 1996, 2168, 2126, 2004, 10578, 1996, 2944, 1012, 102], [101, 8285, 18715, 18595, 6290, 2465, 2097, 6723, 1996, 5372, 19204, 17629, 2465, 1999, 1996, 3075, 2241, 2006, 1996, 26520, 2171, 1012, 102, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]}\n---------------------------------\nmax_length =  {'input_ids': [[101, 10578, 1996, 14324, 19204, 17629, 4738, 2007, 1996, 2168, 26520, 2004, 14324, 2003, 2589, 1996, 2168, 2126, 2004, 10578, 1996, 2944, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 8285, 18715, 18595, 6290, 2465, 2097, 6723, 1996, 5372, 19204, 17629, 2465, 1999, 1996, 3075, 2241, 2006, 1996, 26520, 2171, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n---------------------------------\nspecified max length =  {'input_ids': [[101, 10578, 1996, 14324, 19204, 17629, 4738, 2007, 1996, 2168, 26520, 2004, 14324, 2003, 2589, 1996, 2168, 2126, 2004, 10578, 1996, 2944, 1012, 102], [101, 8285, 18715, 18595, 6290, 2465, 2097, 6723, 1996, 5372, 19204, 17629, 2465, 1999, 1996, 3075, 2241, 2006, 1996, 26520, 2171, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Will truncate the sequences that are longer than the model max length\n# (512 for BERT or DistilBERT)\nmodel_inputs = tokenizer(sequences, truncation=True)\nprint(\"truncate = \", model_inputs)\nprint(\"---------------------------------\")\n\n# Will truncate the sequences that are longer than the specified max length\nmodel_inputs = tokenizer(sequences, max_length=8, truncation=True)\nprint(\"truncate specified max length = \", model_inputs)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T14:14:46.690986Z","iopub.execute_input":"2024-07-12T14:14:46.691423Z","iopub.status.idle":"2024-07-12T14:14:46.700296Z","shell.execute_reply.started":"2024-07-12T14:14:46.691388Z","shell.execute_reply":"2024-07-12T14:14:46.698890Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"truncate =  {'input_ids': [[101, 10578, 1996, 14324, 19204, 17629, 4738, 2007, 1996, 2168, 26520, 2004, 14324, 2003, 2589, 1996, 2168, 2126, 2004, 10578, 1996, 2944, 1012, 102], [101, 8285, 18715, 18595, 6290, 2465, 2097, 6723, 1996, 5372, 19204, 17629, 2465, 1999, 1996, 3075, 2241, 2006, 1996, 26520, 2171, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n---------------------------------\ntruncate specified max length =  {'input_ids': [[101, 10578, 1996, 14324, 19204, 17629, 4738, 102], [101, 8285, 18715, 18595, 6290, 2465, 2097, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Returns PyTorch tensors\nmodel_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\nprint(\"PyTorch = \", model_inputs)\nprint(\"---------------------------------\")\n\n# Returns TensorFlow tensors\nmodel_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\nprint(\"TensorFlow = \", model_inputs)\nprint(\"---------------------------------\")\n\n# Returns NumPy arrays\nmodel_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")\nprint(\"NumPy = \", model_inputs)\nprint(\"---------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T14:17:29.656294Z","iopub.execute_input":"2024-07-12T14:17:29.657395Z","iopub.status.idle":"2024-07-12T14:17:29.672115Z","shell.execute_reply.started":"2024-07-12T14:17:29.657354Z","shell.execute_reply":"2024-07-12T14:17:29.670723Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"PyTorch =  {'input_ids': tensor([[  101, 10578,  1996, 14324, 19204, 17629,  4738,  2007,  1996,  2168,\n         26520,  2004, 14324,  2003,  2589,  1996,  2168,  2126,  2004, 10578,\n          1996,  2944,  1012,   102],\n        [  101,  8285, 18715, 18595,  6290,  2465,  2097,  6723,  1996,  5372,\n         19204, 17629,  2465,  1999,  1996,  3075,  2241,  2006,  1996, 26520,\n          2171,  1012,   102,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n---------------------------------\nTensorFlow =  {'input_ids': <tf.Tensor: shape=(2, 24), dtype=int32, numpy=\narray([[  101, 10578,  1996, 14324, 19204, 17629,  4738,  2007,  1996,\n         2168, 26520,  2004, 14324,  2003,  2589,  1996,  2168,  2126,\n         2004, 10578,  1996,  2944,  1012,   102],\n       [  101,  8285, 18715, 18595,  6290,  2465,  2097,  6723,  1996,\n         5372, 19204, 17629,  2465,  1999,  1996,  3075,  2241,  2006,\n         1996, 26520,  2171,  1012,   102,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 24), dtype=int32, numpy=\narray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 0]], dtype=int32)>}\n---------------------------------\nNumPy =  {'input_ids': array([[  101, 10578,  1996, 14324, 19204, 17629,  4738,  2007,  1996,\n         2168, 26520,  2004, 14324,  2003,  2589,  1996,  2168,  2126,\n         2004, 10578,  1996,  2944,  1012,   102],\n       [  101,  8285, 18715, 18595,  6290,  2465,  2097,  6723,  1996,\n         5372, 19204, 17629,  2465,  1999,  1996,  3075,  2241,  2006,\n         1996, 26520,  2171,  1012,   102,     0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 0]])}\n---------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"# Wrapping up: From tokenizer to model\n# Now that weâ€™ve seen all the individual steps the tokenizer object uses when applied on texts, \n# letâ€™s see one final time how it can handle multiple sequences (padding!), very long sequences (truncation!), \n# and multiple types of tensors with its main API:\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\nsequences = [\"Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model.\", \n            \"AutoTokenizer class will grab the proper tokenizer class in the library based on the checkpoint name.\"]\n\ntokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\noutput = model(**tokens)\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T14:23:25.540998Z","iopub.execute_input":"2024-07-12T14:23:25.541770Z","iopub.status.idle":"2024-07-12T14:23:26.359177Z","shell.execute_reply.started":"2024-07-12T14:23:25.541730Z","shell.execute_reply":"2024-07-12T14:23:26.358039Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3540, -2.8169],\n        [ 3.0878, -2.5283]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}